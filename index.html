<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
	<style>
		body{
			color: #333;
		}
		#container{
			min-width: 1000px;
			width: 1000px;
/*			overflow: auto;
*/			margin: 50px auto;padding: 30px;
			/*zoom: 1;*/
*//*			border: 1px solid #ccc;background: #fc9;color: #fff;
*/		}
		#left{
			float: left;
			width: 230px;
			height: 250px;
			margin-left: 0px;
		}
		#right{
			float: left;
			width: auto;
			margin-left: 30px;
		}
		#name{
			font-size: 22.0pt;
		    mso-bidi-font-size: 24.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		        font-weight: bold;
		}
		#info{
		    font-size: 16.0pt;
		    mso-bidi-font-size: 16.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 30px;
		    margin-left: 5px;
		    margin-bottom: 10px;
		    
		}
		.clear{clear:both; height: 0; line-height: 0; font-size: 0}
		.Bio{
			font-size:16.0pt;
			mso-bidi-font-size:17.0pt;
			line-height:150%;
			font-family:Times;
			mso-bidi-font-family:Lato-Regular;
			text-align: justify;
		}
		span.SpellE {
		    mso-style-name: "";
		    mso-spl-e: yes;
		}
		span.Title{
			    font-size: 22.0pt;
			    mso-bidi-font-size: 17.0pt;
			    font-family: Times;
			    mso-bidi-font-family: Lato-Regular;
			    font: bold;
			    margin-top: 10px;
			    width: 1000px;
		}
		div.section{
			padding-top: 30px;
		}
		
		div.sub-left{
			float: left;
			width: 250px;
						
		}
		div.sub-left img{
			vertical-align: middle;
			horizontal-align: middle;
			margin-top: 10px;
		}
		
		div.sub-left span{
			height: 100%;
			display: inline-block;
			vertical-align: top;
						
		}
		div.sub-right{
			float: left;
			width: 750px;			
		}
		.paper{
			overflow: auto;
			zoom:1;
			border-top: 2px ;
			padding-bottom: 0px;
			min-height: 160px;

		}
		.paperTitle{
			font-size:14pt;
			mso-bidi-font-size:14pt;
			font-family:Calibri;
			mso-bidi-font-family:Calibri;
			margin-top: 10px;
			margin-bottom: 5px;
			font-weight: bold;
		}
		.paperName{
		    font-size: 12pt;
		    mso-bidi-font-size: 12pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;
		    line-height:160%;
		    font-style: italic;
		}		
		.paperPub{
		    font-size: 14pt;
		    mso-bidi-font-size: 14pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;		    
		    font-style: italic;
		    line-height:160%;
		}
		.paperLink{
		    font-size: 13.0pt;
		    mso-bidi-font-size: 13.0pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;
		    line-height:170%;
		}
		.special{
		    margin-top: 0in;
		    margin-bottom: 0in;
		    margin-left: -.9pt;
		    margin-bottom: .0001pt;
		    text-indent: .9pt;
		    mso-pagination: none;
		    tab-stops: 13.75in;
		    mso-layout-grid-align: none;
		    text-autospace: none;
		}
		.long div.sub-left, .long div.sub-right{
			height: 300px;
			width: 980px;

		}
		.short div.sub-left, .short div.sub-right{
			height:150px;

		}
		div.sub-left,div.sub-right{
			height:200px;

		}
	</style>
</head>
<body>
	<div id="container">
		<div id="left">			
			<img width="200" height="230" src="imgs/photo5.jpg">
		</div>
		<div id="right">
			<div id="name">Yingqian Wang (王应谦) </div>
			<div id="info">

				Assistant Professor<p>
				National University of Defense Technology (NUDT)<p>
				Email: wangyingqian16@nudt.edu.cn<p>				
			</div>
			         <a href="https://www.researchgate.net/profile/Yingqian_Wang3?ev=prf_highl" target="_blank" rel="nofollow"><span>Research Gate</span></a>  |
			         <a href="https://github.com/YingqianWang" target="_blank" rel="nofollow"><span>Github</span></a>  |
				 <a href="https://scholar.google.com/citations?user=tBA4alMAAAAJ&hl=zh-CN" target="_blank" rel="nofollow"><span>Google Scholar</span></a>  
			
			</div>

		<div class="clear"></div>
		<div class="section">
			<span class="Title"><b>Brief Bio</b></span><p>			
				<div class="Bio">
					I received my Ph.D. and Master degrees from NUDT in 2023 and 2018, respectively. 
					Before that, I received my B.E. degree from Shandong University in 2016.
					Currently, I'm an assistant professor with the College of Electronic Science and Technology, NUDT.
					My research interests focus on optical imaging and detection, particularly on
					<b style="mso-bidi-font-weight:normal">light field imaging</b> and <b style="mso-bidi-font-weight:normal">image super-resolution</b>.</span></p>
				</div>


	<div class="section">
		<span class="Title"><b>News</b></span><p>
		<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		<div class="paper long"><b>
			<div class="sub-right">
			<div class="paperName"><b>	
			2024.05 | Our paper "Disentangling Light Fields for Super-Resolution and Disparity Estimation" is selected as an <span style="color:red">ESI Hot Paper</span>.<br>
			2024.05 | Our paper "Dense Nested Attention Network for Infrared Small Target Detection" is selected as an <span style="color:red">ESI Hot Paper</span>.<br>
			2024.05 | One paper on unpaired image and point cloud restoration is accepted by <span style="color:red">IEEE TPAMI</span>.<br>
			2024.03 | Our paper "Real-World Light Field Image Super-Resolution via Degradation Modulation" is accepted by <span style="color:red">IEEE TNNLS</span>.<br>
			2024.02 | Our paper "Learning Coupled Dictionaries from Unpaired Data for Image Super-Resolution" is accepted to <span style="color:red">CVPR 2024</span>.<br>
			2023.11 | One paper on multi-frame infrared small target detection is accepted to <span style="color:red">IEEE TNNLS</span>.<br>
			2023.09 | Four papers are selected as <span style="color:red">Highly Cited Papers</span> in the latest issue of ESI Index.<br>
			2023.07 | Two papers on light field image super-resolution and pointly supervised infrared small target detection are accepted to <span style="color:red">ICCV 2023</span>.<br>
			2023.02 | One paper on pointly supervised infrared small target detection is accepted to <span style="color:red">CVPR 2023</span>.<br>
			2023.01 | We are organizing <a href="https://github.com/The-Learning-And-Vision-Atelier-LAVA/Stereo-Image-SR/tree/NTIRE2023" target="_blank" rel="nofollow">NTIRE Stereo Image SR Challenge</a> and <a href="https://github.com/The-Learning-And-Vision-Atelier-LAVA/LF-Image-SR/tree/NTIRE2023" target="_blank" rel="nofollow">NTIRE LF Image SR Challenge</a> at CVPR 2023.<br>	
			2022.07 | Our paper "Dense Nested Attention Network for Infrared Small Target Detection" is accepted by <span style="color:red">IEEE TIP</span>. <br>
			2022.07 | Our paper "Exploring Fine-Grained Sparsity in Convolutional Neural Networks for Efficient Inference" is accepted by <span style="color:red">IEEE TPAMI</span>.<br>
			2022.03 | Two papers on network quantization and light field depth estimation are accepted to <span style="color:red">CVPR 2022</span>.<br>
			2022.02 | Our paper "Disentangling Light Fields for Super-Resolution and Disparity Estimation" is accepted by <span style="color:red">IEEE TPAMI</span>.<br>
			2022.01 | We are organizing <a href="https://github.com/The-Learning-And-Vision-Atelier-LAVA/Stereo-Image-SR/tree/NTIRE2022" target="_blank" rel="nofollow">NTIRE Stereo Image Super-Resolution Challenge</a> at CVPR 2022.<br> 
			2021.10 | Our paper "Dense Dual-Attention Network for Light Field Image Super-Resolution" is accepted by IEEE TCSVT. [<a href="https://arxiv.org/pdf/2110.12114.pdf" target="_blank" rel="nofollow">pdf</a>]<br>				
			2021.10 | Our paper "Spatial-Angular Attention Network for Light Field Reconstruction" is accepted by <span style="color:red">IEEE TIP</span>. <br>	
			2021.07 | Our paper "Learning a Single Network for Scale-Arbitrary Super-Resolution" is accepted to <span style="color:red">ICCV 2021</span>.<br>
			2021.03 | Two papers on single image super-resolution are accepted to <span style="color:red">CVPR 2021</span>.<br>
			2020.11 | Our paper "Light Field Image Super-Resolution Using Deformable Convolution" is accepted by <span style="color:red">IEEE TIP</span>.<br>
			2020.09 | An online tutorial (120 min in Chinese) regarding our Parallax Attention Mechanism is available <a href="https://www.shenlanxueyuan.com/open/course/77" target="_blank" rel="nofollow">here</a>.<br>
			2020.09 | Our paper "Parallax Attention for Unsupervised Stereo Correspondence Learning" is accepted by <span style="color:red">IEEE TPAMI</span>.<br>
			2020.07 | Our paper "Spatial-Angular Interaction for Light Field Image Super-Resolution" is accepted to <span style="color:red">ECCV 2020</span>.<br>	
			2019.12 | Our paper "DeOccNet: Learning to See Through Foreground Occlusions in Light Fields" is accepted to WACV 2020.<br>
			2019.03 | A large-scale dataset for stereo image super-resolution is available online at <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">Flickr1024</a>. <br>
			2019.02 | Our paper "Learning Parallax Attention for Stereo Image Super-Resolution" is accepted to <span style="color:red">CVPR 2019</span>.<br><br>
			</b></div>
			</b></div>
		</b></div>
	</div>
	
	
	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Publications --- 2024</b></span><p><p>
	<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/UDRL.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Unsupervised Degradation Representation Learning for Unpaired Restoration of Images and Point Clouds
					</div>
					<div class="paperName">
						Longguang Wang, Yulan Guo, <b>Yingqian Wang</b>, Xiaoyu Dong, Qingyu Xu, Jungang Yang, Wei An
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>IEEE TPAMI</b></span>, 2024.<br> 
					</div>
					<div class="paperLink">
						| <a href="" target="_blank" rel="nofollow">Paper</a>
						| <a href="" target="_blank" rel="nofollow">Code</a>						
					</div>
				</div>
			</div>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/DictSR.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Learning Coupled Dictionaries from Unpaired Data for Image Super-Resolution
					</div>
					<div class="paperName">
						Longguang Wang, Juncheng Li, <b>Yingqian Wang</b>, Qingyong Hu, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>CVPR</b></span>, 2024.<br> 
					</div>
					<div class="paperLink">
						| <a href="" target="_blank" rel="nofollow">Paper</a>							
						| <a href="" target="_blank" rel="nofollow">Code</a>
					</div>
				</div>
			</div>

		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/LF-DMnet_demo.gif">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Real-World Light Field Image Super-Resolution via Degradation Modulation
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Zhengyu Liang, Longguang Wang, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>TNNLS</b></span>, 2024.<br> 
					</div>
					<div class="paperLink">
						| <a href="https://yingqianwang.github.io/LF-DMnet" target="_blank" rel="nofollow">Project Page</a>
						| <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10486857" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://github.com/YingqianWang/LF-DMnet" target="_blank" rel="nofollow">Code</a>
						
					</div>
				</div>
			</div>			
		
	
	
	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Publications --- 2023</b></span><p><p>
	<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/EPIT.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Learning Non-Local Spatial-Angular Correlation for Light Field Image Super-Resolution
					</div>
					<div class="paperName">
						Zhengyu Liang, <b>Yingqian Wang</b>, Longguang Wang, Jungang Yang, Shilin Zhou, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>ICCV</b></span>, 2023.<br> 
					</div>
					<div class="paperLink">
						| <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_Learning_Non-Local_Spatial-Angular_Correlation_for_Light_Field_Image_Super-Resolution_ICCV_2023_paper.pdf" target="_blank" rel="nofollow">Paper</a>	
						| <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Liang_Learning_Non-Local_Spatial-Angular_ICCV_2023_supplemental.pdf" target="_blank" rel="nofollow">Supp</a>	
						| <a href="https://zhengyuliang24.github.io/EPIT/" target="_blank" rel="nofollow">Webpage</a>		
						| <a href="https://github.com/ZhengyuLiang24/EPIT" target="_blank" rel="nofollow">Code</a>
					</div>
				</div>
			</div>
		
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/MCLC.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Monte Carlo Linear Clustering with Single-Point Supervision is Enough for Infrared Small Target Detection
					</div>
					<div class="paperName">
						Boyang Li, <b>Yingqian Wang</b>, Longguang Wang, Fei Zhang, Ting Liu, Zaiping Lin, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>ICCV</b></span>, 2023.<br> 
					</div>
					<div class="paperLink">
						| <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Monte_Carlo_Linear_Clustering_with_Single-Point_Supervision_is_Enough_for_ICCV_2023_paper.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Li_Monte_Carlo_Linear_ICCV_2023_supplemental.zip" target="_blank" rel="nofollow">Supp</a>
						| <a href="https://yeren123455.github.io/SIRST-Single-Point-Supervision/" target="_blank" rel="nofollow">Webpage</a>						
						| <a href="https://github.com/YeRen123455/SIRST-Single-Point-Supervision" target="_blank" rel="nofollow">Code</a>	
					</div>
				</div>
			</div>
		

		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/LESPS.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Mapping Degeneration Meets Label Evolution: Learning Infrared Small Target Detection with Single Point Supervision
					</div>
					<div class="paperName">
						Xinyi Ying, Li Liu, <b>Yingqian Wang</b>, Ruojing Li, Nuo Chen, Zaiping Lin, Weidong Sheng, Shilin Zhou
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>CVPR</b></span>, 2023.<br> 
					</div>
					<div class="paperLink">
						| <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ying_Mapping_Degeneration_Meets_Label_Evolution_Learning_Infrared_Small_Target_Detection_CVPR_2023_paper.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Ying_Mapping_Degeneration_Meets_CVPR_2023_supplemental.pdf" target="_blank" rel="nofollow">Supp</a>
						| <a href="https://xinyiying.github.io/LESPS/" target="_blank" rel="nofollow">Webpage</a>						
						| <a href="https://github.com/XinyiYing/LESPS" target="_blank" rel="nofollow">Code</a>	
					</div>
				</div>
			</div>
		
		
		
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/NTIRE-LFSR.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						NTIRE 2023 Challenge on Light Field Image Super-Resolution: Dataset, Methods and Results
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Longguang Wang, Zhengyu Liang, Jungang Yang, Radu Timofte, Yulan Guo et al.
					</div>
					<div class="paperPub">
						CVPRW, 2023.<br>
					</div>
					<div class="paperLink">
						| <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Wang_NTIRE_2023_Challenge_on_Light_Field_Image_Super-Resolution_Dataset_Methods_CVPRW_2023_paper.pdf" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://wyqdatabase.s3.us-west-1.amazonaws.com/NTIRE_2023.mp4" target="_blank" rel="nofollow">Video</a>
						| <a href="https://codalab.lisn.upsaclay.fr/competitions/9201" target="_blank" rel="nofollow">CodaLab</a>
						| <a href="https://github.com/The-Learning-And-Vision-Atelier-LAVA/LF-Image-SR/tree/NTIRE2023" target="_blank" rel="nofollow">Github</a>
						| <a href="https://github.com/ZhengyuLiang24/BasicLFSR" target="_blank" rel="nofollow">BasicLFSR Toolbox</a>
					</div>
				</div>
			</div>
		
	
			
			
	
	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Publications --- 2022</b></span><p><p>
	<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/DistgLF.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Disentangling Light Fields for Super-Resolution and Disparity Estimation
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Longguang Wang, Gaochang Wu, Jungang Yang, Wei An, Jingyi Yu, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>IEEE TPAMI</b></span>, 2022. (<span style="color:red">ESI Hot Paper</span>)<br>
					</div>
					<div class="paperLink">
						| <a href="https://arxiv.org/pdf/2202.10603.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://yingqianwang.github.io/DistgLF/" target="_blank" rel="nofollow">Project Page</a>
						| <a href="https://mp.weixin.qq.com/s/T3D0TGZN2fZbsRPV1U147A" target="_blank" rel="nofollow">News</a>
						| <a href="https://github.com/YingqianWang/DistgSSR" target="_blank" rel="nofollow">DistgSSR</a>
						| <a href="https://github.com/YingqianWang/DistgASR" target="_blank" rel="nofollow">DistgASR</a>
						| <a href="https://github.com/YingqianWang/DistgDisp" target="_blank" rel="nofollow">DistgDisp</a>
					</div>
				</div>
			</div>
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/SMCNN.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Exploring Fine-Grained Sparsity in Convolutional Neural Networks for Efficient Inference
					</div>
					<div class="paperName">
						Longguang Wang, Yulan Guo, Xiaoyu Dong, <b>Yingqian Wang</b>, Xinyi Ying, Zaiping Lin, Wei An
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>IEEE TPAMI</b></span>, 2022.<br>
					</div>
					<div class="paperLink">
						| <a href="https://ieeexplore.ieee.org/document/9841044" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://github.com/LongguangWang/SparseMask" target="_blank" rel="nofollow">Code</a>						
					</div>
				</div>
			</div>  	
		
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/OACC-Net.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Occlusion-Aware Cost Constructor for Light Field Depth Estimation
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Longguang Wang, Zhengyu Liang, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>CVPR</b></span>, 2022.<br> 
					</div>
					<div class="paperLink">
						| <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Occlusion-Aware_Cost_Constructor_for_Light_Field_Depth_Estimation_CVPR_2022_paper.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Wang_Occlusion-Aware_Cost_Constructor_CVPR_2022_supplemental.pdf" target="_blank" rel="nofollow">Supp</a>	
						| <a href="https://github.com/YingqianWang/OACC-Net" target="_blank" rel="nofollow">Code</a>						
					</div>
				</div>
			</div>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/LLP.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Learnable Lookup Table for Neural Network Quantization
					</div>
					<div class="paperName">
						Longguang Wang, Xiaoyu Dong, <b>Yingqian Wang</b>, Li Liu, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>CVPR</b></span>, 2022.<br> 
					</div>
					<div class="paperLink">
						| <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Learnable_Lookup_Table_for_Neural_Network_Quantization_CVPR_2022_paper.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Wang_Learnable_Lookup_Table_CVPR_2022_supplemental.pdf" target="_blank" rel="nofollow">Supp</a>	
						| <a href="https://github.com/The-Learning-And-Vision-Atelier-LAVA/LLT" target="_blank" rel="nofollow">Code</a>						
					</div>
				</div>
			</div>		
		
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/DNA-Net.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Dense Nested Attention Network for Infrared Small Target Detection
					</div>
					<div class="paperName">
						Boyang Li, Chao Xiao, Longguang Wang, <b>Yingqian Wang</b>, Zaiping Lin, Miao Li, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>IEEE TIP</b></span>, 2022 (<span style="color:red">ESI Hot Paper</span>)<br> 
					</div>
					<div class="paperLink">
						| <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9864119" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://github.com/YeRen123455/Infrared-Small-Target-Detection" target="_blank" rel="nofollow">Code</a>						
					</div>
				</div>
			</div>
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/LFT_attmap.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Light Field Image Super-Resolution with Transformers
					</div>
					<div class="paperName">
						Zhengyu Liang*, <b>Yingqian Wang*</b>, Longguang Wang, Jungang Yang, Shilin Zhou
					</div>
					<div class="paperPub">
						IEEE SPL, 2022.
					</div>
					<div class="paperLink">
						| <a href="https://arxiv.org/pdf/2108.07597.pdf" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://github.com/ZhengyuLiang24/LFT" target="_blank" rel="nofollow">Code</a>					
					</div>
				</div>
			</div>
		
		
	
	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Publications --- 2021</b></span><p><p>
	<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
			
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/DASR.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Unsupervised Degradation Representation Learning for Blind Super-Resolution
					</div>
					<div class="paperName">
						Longguang Wang, <b>Yingqian Wang</b>, Xiaoyu Dong, Qingyu Xu, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>CVPR</b></span>, 2021.<br> 
					</div>
					<div class="paperLink">
						| <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Unsupervised_Degradation_Representation_Learning_for_Blind_Super-Resolution_CVPR_2021_paper.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Wang_Unsupervised_Degradation_Representation_CVPR_2021_supplemental.pdf" target="_blank" rel="nofollow">Supp</a>
						| <a href="https://www.techbeat.net/talk-info?id=537" target="_blank" rel="nofollow">Video Presentation</a>						
						| <a href="https://mp.weixin.qq.com/s/jmaMObrWwyg6j659tGe-Kw" target="_blank" rel="nofollow">News</a>
						| <a href="https://github.com/LongguangWang/DASR" target="_blank" rel="nofollow">Code</a>						
					</div>
				</div>
			</div>
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/SMSR.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Exploring Sparsity in Image Super-Resolution for Efficient Inference
					</div>
					<div class="paperName">
						Longguang Wang, Xiaoyu Dong, <b>Yingqian Wang</b>, Xinyi Ying, Zaiping Lin, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>CVPR</b></span>, 2021.<br> 
					</div>
					<div class="paperLink">
						| <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Exploring_Sparsity_in_Image_Super-Resolution_for_Efficient_Inference_CVPR_2021_paper.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Wang_Exploring_Sparsity_in_CVPR_2021_supplemental.pdf" target="_blank" rel="nofollow">Supp</a>
						| <a href="https://mp.weixin.qq.com/s/vIacjZk6UvNCxJIdBWY0og" target="_blank" rel="nofollow">News</a>
						| <a href="https://www.bilibili.com/video/BV1eL411x7sy?spm_id_from=333.999.0.0" target="_blank" rel="nofollow">Video Presentation</a>
						| <a href="https://github.com/LongguangWang/SMSR" target="_blank" rel="nofollow">Code</a>						
					</div>
				</div>
			</div>	
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="https://longguangwang.github.io/imgs/ArbSR.gif" width="200" height="130">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Learning a Single Network for Scale-Arbitrary Super-Resolution
					</div>
					<div class="paperName">
						Longguang Wang, <b>Yingqian Wang</b>, Zaiping Lin, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>ICCV</b></span>, 2021.<br> 
					</div>
					<div class="paperLink">
						| <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Learning_a_Single_Network_for_Scale-Arbitrary_Super-Resolution_ICCV_2021_paper.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://longguangwang.github.io/Project/ArbSR/" target="_blank" rel="nofollow">Project Page</a>
						| <a href="https://replicate.ai/longguangwang/arbsr" target="_blank" rel="nofollow">Online Demo</a>
						| <a href="https://mp.weixin.qq.com/s/rDtxbt3OPN1wrSe406mVRg" target="_blank" rel="nofollow">News</a>
						| <a href="https://github.com/LongguangWang/Scale-Arbitrary-SR" target="_blank" rel="nofollow">Code</a> 												
					</div>
				</div>
			</div>
		
		
			<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/SAAN.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Spatial-Angular Attention Network for Light Field Reconstruction
					</div>
					<div class="paperName">
						Gaochang Wu, <b>Yingqian Wang</b>, Yebin Liu, Lu Fang, Tianyou Chai
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>IEEE TIP</b></span>, 2021.<br>
					</div>
					<div class="paperLink">
						| <a href="https://arxiv.org/pdf/2007.02252v2.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://gaochangwu.github.io/SAAN/SAA-Net.html" target="_blank" rel="nofollow">Project Page</a>						
						| <a href="https://github.com/GaochangWu/SAAN" target="_blank" rel="nofollow">Code</a>						
					</div>
				</div>
			</div>


		
	
	
	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Publications --- 2020</b></span><p><p>
	<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		

                <div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/PASMnet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Parallax Attention for Unsupervised Stereo Correspondence Learning
					</div>
					<div class="paperName">
						Longguang Wang, Yulan Guo, <b>Yingqian Wang</b>, Zhengfa Liang, Zaiping Lin, Jungang Yang, Wei An
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>IEEE TPAMI</b></span>, 2020 (ESI Highly Cited Paper)<br>
					</div>
					<div class="paperLink">
						| <a href="https://arxiv.org/pdf/2009.08250.pdf" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://www.shenlanxueyuan.com/open/course/77" target="_blank" rel="nofollow">Tutorial</a>
						| <a href="https://mp.weixin.qq.com/s/EdEwOEm5ttj3IM-6jI_s4A" target="_blank" rel="nofollow">News</a>						
						| <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">Dataset</a>
						| <a href="https://github.com/LongguangWang/PAM" target="_blank" rel="nofollow">Code</a>						
					</div>
				</div>
			</div>	
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/LF-DFnet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Light Field Image Super-Resolution Using Deformable Convolution
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Jungang Yang, Longguang Wang, Xinyi Ying, Tianhao Wu, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>IEEE TIP</b></span>, 2020.<br>
					</div>
					<div class="paperLink">
						| <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286855" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://github.com/YingqianWang/LF-DFnet" target="_blank" rel="nofollow">Code</a>						
					</div>
				</div>
			</div>
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/SFEAFE.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Spatial-Angular Interaction for Light Field Image Super-Resolution
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Longguang Wang, Jungang Yang, Wei An, Jingyi Yu, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>ECCV</b></span>, 2020.<br>
					</div>
					<div class="paperLink">
						| <a href="https://arxiv.org/pdf/1912.07849.pdf" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&mid=2247520164&idx=1&sn=beb7031162b5c173e66a77b01a828e5b&chksm=96f0fdf0a18774e6a9f85e316efd64d9990dca2213e353c2da373b0a6f4004717e9b092923fb&mpshare=1&srcid=07057miMA6psG3L6UHJZOYb1&sharer_sharetime=1593964266602&sharer_shareid=eadfa6ebb7f5bf94747b471d67269b5e&from=timeline&scene=2&subscene=1&clicktime=1593965476&enterid=1593965476&ascene=14&devicetype=Windows+10+x64&version=62090529&nettype=3gnet&abtest_cookie=AAACAA%3D%3D&lang=zh_CN&exportkey=AbY3Hk%2Bj7%2FZr1bhOoseHkrg%3D&pass_ticket=xxF82PuauoRDV3pp3gMzbUWag4Wn9ibURKETDRhWZzgqvAuhhLZpHUAsxPEkeaTP&wx_header=1&key=d197f1f59dba99c28d4c1b99b04b6bd1077e51e74b63c15b1c47bf892712247f8c43b777c67399686f47ac2805132c5cecb7dd7d8fcd8ede07661f7a2dc7733f6be335fc77d4b5033daad98428a3d3ef&uin=MTM2MDA1MjgzOA%3D%3D" target="_blank" rel="nofollow">News</a>
						| <a href="https://wyqdatabase.s3-us-west-1.amazonaws.com/LF-InterNet.mp4" target="_blank" rel="nofollow">Presentation</a>
						| <a href="https://github.com/YingqianWang/LF-InterNet" target="_blank" rel="nofollow">Code</a>						
					</div>
				</div>
			</div>
		
				
			<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="130" src="imgs/DeOccNet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						DeOccNet: Learning to See Through Foreground Occlusions in Light Fields
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Tianhao Wu, Jungang Yang, Longguang Wang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						WACV, 2020.<br>
					</div>
					<div class="paperLink">
						| <a href="https://arxiv.org/pdf/1912.04459.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://mp.weixin.qq.com/s/0K_NF84wvPJttEARVUGPWA" target="_blank" rel="nofollow">News</a>	
						| <a href="https://yingqianwang.github.io/DeOccNet/Poster.pdf" target="_blank" rel="nofollow">Poster</a>
						| <a href="https://github.com/YingqianWang/DeOccNet" target="_blank" rel="nofollow">Code&Dataset</a>					
					</div>
				</div>
			</div>	
		
		


	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Publications --- 2019</b></span><p><p>
	<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="120" src="imgs/PASSRnet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Learning Parallax Attention for Stereo Image Super-Resolution
					</div>
					<div class="paperName">
						Longguang Wang, <b>Yingqian Wang</b>, Zhengfa Liang, Zaiping Lin, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>CVPR</b></span>, 2019.<br> 
					</div>
					<div class="paperLink">
						| <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Learning_Parallax_Attention_for_Stereo_Image_Super-Resolution_CVPR_2019_paper.pdf" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">Dataset</a>
						| <a href="https://mp.weixin.qq.com/s/zN11cI3dOOp1PDXaCPcRng" target="_blank" rel="nofollow">News</a>
						| <a href="https://github.com/LongguangWang/PASSRnet" target="_blank" rel="nofollow">Code</a>					
					</div>
				</div>
			</div>
		
		
			<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="130" src="imgs/Flickr1024.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Flickr1024: A Large-Scale Dataset for Stereo Image Super-Resolution
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Longguang Wang, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						ICCV Workshop, 2019.<br>
					</div>
					<div class="paperLink">
						| <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/LCI/Wang_Flickr1024_A_Large-Scale_Dataset_for_Stereo_Image_Super-Resolution_ICCVW_2019_paper.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">Dataset</a>
					</div>
				</div>
			</div>		
		
			
		
<div class="section">
				<span class="Title"><b>Academic Services</b></span><p>
				<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
				<div class="paperName"><b>
					PC Members:<br>
					<a href="https://cvlai.net/ntire/2023/" target="_blank" rel="nofollow">New Trends in Image Restoration and Enhancement (NTIRE) Wokshop @ CVPR 2023</a>,	<br>				
					<a href="https://data.vision.ee.ethz.ch/cvl/aim22/" target="_blank" rel="nofollow">Advances in Image Manipulation (AIM) Workshop @ ECCV 2022</a>,<br>
					<a href="https://data.vision.ee.ethz.ch/cvl/ntire22/" target="_blank" rel="nofollow">New Trends in Image Restoration and Enhancement (NTIRE) Wokshop @ CVPR 2022</a>,<br>					
					<br>
					Challenge Organization:<br>
					<a href="https://codalab.lisn.upsaclay.fr/competitions/9201" target="_blank" rel="nofollow">The 1st Light Field Image Super-Resolution Challenge @ NTIRE 2023</a>,<br>
					<a href="https://codalab.lisn.upsaclay.fr/competitions/10047" target="_blank" rel="nofollow">The 2nd Stereo Image Super-Resolution Challenge @ NTIRE 2023</a>,<br>
					<a href="https://codalab.lisn.upsaclay.fr/competitions/1598" target="_blank" rel="nofollow">The 1st Stereo Image Super-Resolution Challenge @ NTIRE 2022</a>,<br>
					<br>
					Conference Reviewer:<br>
					<a href="https://cvpr2021.thecvf.com/" target="_blank" rel="nofollow">CVPR 2021</a>,
					<a href="https://cvpr2022.thecvf.com/" target="_blank" rel="nofollow"> CVPR 2022</a>,
					<a href="https://cvpr2023.thecvf.com/" target="_blank" rel="nofollow">CVPR 2023</a>,
					<a href="https://cvpr.thecvf.com/" target="_blank" rel="nofollow">CVPR 2024</a><br>		
					<a href="https://eccv2022.ecva.net/" target="_blank" rel="nofollow">ECCV 2022</a>,
					<a href="https://eccv2024.ecva.net/" target="_blank" rel="nofollow">ECCV 2024</a><br>
					<a href="https://iccv2021.thecvf.com/" target="_blank" rel="nofollow">ICCV 2021</a>,
					<a href="https://iccv2023.thecvf.com/" target="_blank" rel="nofollow">ICCV 2023</a><br>
					<a href="https://neurips.cc/" target="_blank" rel="nofollow">NeurIPS 2023</a>,
					<a href="https://neurips.cc/" target="_blank" rel="nofollow">NeurIPS 2024</a><br>
					<a href="https://iclr.cc/" target="_blank" rel="nofollow">ICLR 2024</a><br>
					<a href="https://aaai-23.aaai.org/" target="_blank" rel="nofollow">AAAI 2023</a>, 
					<a href="https://aaai.org/aaai-conference/" target="_blank" rel="nofollow">AAAI 2024</a><br>
					<a href="https://2021.acmmm.org/" target="_blank" rel="nofollow">ACM MM 2021</a>, 
					<a href="https://2022.acmmm.org/" target="_blank" rel="nofollow">ACM MM 2022</a>,
					<a href="https://2023.acmmm.org/" target="_blank" rel="nofollow">ACM MM 2023</a>,
					<a href="https://2024.acmmm.org/" target="_blank" rel="nofollow">ACM MM 2024</a><br>
					......<br>
					<br>
					Journal Reviewer:<br>
					<a href="https://www.springer.com/journal/11263" target="_blank" rel="nofollow">International Journal of Computer Vision</a> <br>					
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83" target="_blank" rel="nofollow">IEEE Transactions on Image Processing</a> <br>
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046" target="_blank" rel="nofollow">IEEE Transactions on Multimedia</a> <br>
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76" target="_blank" rel="nofollow">IEEE Transactions on Circuits and Systems for Video Technology</a> <br>
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6745852" target="_blank" rel="nofollow">IEEE Transactions on Computational Imaging</a> <br>
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36" target="_blank" rel="nofollow">IEEE Transactions on Geoscience and Remote Sensing</a> <br>
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=11" target="_blank" rel="nofollow">IEEE Transactions on Broadcasting</a> <br>
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=19" target="_blank" rel="nofollow">IEEE Transactions on Instrumentation and Measurement</a> <br>
					<a href="https://www.sciencedirect.com/journal/isprs-journal-of-photogrammetry-and-remote-sensing" target="_blank" rel="nofollow">ISPRS Journal of Photogrammetry and Remote Sensing</a> <br>
					<a href="https://www.sciencedirect.com/journal/international-journal-of-applied-earth-observation-and-geoinformation?utm_campaign=STMJ_1636705839_SC&utm_medium=SRCH&utm_source=B&dgcid=STMJ_1636705839_SC" target="_blank" rel="nofollow">International Journal of Applied Earth Observation and Geoinformation</a> <br>
					<a href="https://www.journals.elsevier.com/pattern-recognition-letters" target="_blank" rel="nofollow">Pattern Recognition Letters</a> <br>
					<a href="https://www.sciencedirect.com/journal/information-fusion" target="_blank" rel="nofollow">Information Fusion</a> <br>					
					......<br>
					<br>
					</b></div>
			</div>		
		
		
			
		

			<div class="section">
				<span class="Title"><b>Teaching Assistance</b></span><p>
				<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
				<div class="paperName"><b>					
					Lecture: Optical Imaging and Detection (Spring Term, 2021)<br>
					Lecture: Optical Imaging and Detection (Autumn Term, 2020)<br>
					Lecture: Signals and Systems (Spring Term, 2020)<br>
					Lecture: Target Detection and Signal Processing (Autumn Term, 2019)<br>
					Lecture: Target Detection and Signal Processing (Autumn Term, 2018)<br>
				</b></div>
			</div>
		
		
			
		

			<div class="section">
				<span class="Title"><b>Awards & Honors</b></span><p>
				<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
				<div class="paperName"><b>
					2024 | Outstanding PhD Thesis Award of NUDT<br>
					2023 | Excellent Doctoral Graduates of NUDT (5 over 228)<br>
					2022 | First-class Scholarship of NUDT<br>
					2021 | Outstanding Master Dissertation Award of Hunan Province<br>
					2018 | Guanghua Scholarship<br>
					2016 | Excellent Graduates of Shandong Province<br>
					2015 | The 1st Prize in the Final of China Mathematics Competitions (45 winners over 63K participants, Top 0.07%)<br>	
					2015 | National Scholarship (Ministry of Education, Top 2%)<br>								
					2014 | National Scholarship (Ministry of Education, Top 2%)<br>
					2013 | National Scholarship (Ministry of Education, Top 2%)<br>
				</b></div>
			</div>		
		
			<!-- site visitors begjin -->
			<div style="margin:50px 0;">
				<a href="https://clustrmaps.com/site/1bffo" title="Visit tracker"><img src="//clustrmaps.com/map_v2.png?cl=ffffff&w=500&t=tt&d=ueKrfCS3qabq9AqNETgGVXDkjNud6pEFK3nRS1f1NxQ" /></a>
			</div>
			<!-- site visitors end -->
		
			<!-- Last update time begjin -->
			<div style="border-top: 3px solid #555; text-align: center;">
				<p style="color: #555;">Last updated: 2023-07-23</p>
			</div>
			<!-- Last update time end -->

	</div>
	
</body>
</html>
